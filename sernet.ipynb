{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sernet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TonyNgcn/meme-classification/blob/master/sernet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0woP8PB0-m44",
        "colab_type": "text"
      },
      "source": [
        "There are two different kind of GPU in colab，K80 or T4. K80 has only 11G+ memory while T4 have 15G memory. K80 can't run this program. If you get a K80 GPU, you should close this page and wait until you get a T4 GPU.  \n",
        "\n",
        "The following cell is for you to check your GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2AMJ1HlVTAw",
        "colab_type": "code",
        "outputId": "2cc21802-a932-464b-f418-439199977be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 14 03:00:06 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    17W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k85wbzGbzs_G",
        "colab_type": "text"
      },
      "source": [
        "The following two cell is for the original Cifar10 dataset, copied from https://github.com/taki0112/SENet-Tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSq69rqYOGF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class_num = 10\n",
        "image_size = 32\n",
        "img_channels = 3\n",
        "\n",
        "\n",
        "# ========================================================== #\n",
        "# ├─ prepare_data()\n",
        "#  ├─ download training data if not exist by download_data()\n",
        "#  ├─ load data by load_data()\n",
        "#  └─ shuffe and return data\n",
        "# ========================================================== #\n",
        "\n",
        "\n",
        "\n",
        "def download_data():\n",
        "    dirname = 'cifar-10-batches-py'\n",
        "    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "    fname = 'cifar-10-python.tar.gz'\n",
        "    fpath = './' + dirname\n",
        "\n",
        "    download = False\n",
        "    if os.path.exists(fpath) or os.path.isfile(fname):\n",
        "        download = False\n",
        "        print(\"DataSet aready exist!\")\n",
        "    else:\n",
        "        download = True\n",
        "    if download:\n",
        "        print('Downloading data from', origin)\n",
        "        import urllib.request\n",
        "        import tarfile\n",
        "\n",
        "        def reporthook(count, block_size, total_size):\n",
        "            global start_time\n",
        "            if count == 0:\n",
        "                start_time = time.time()\n",
        "                return\n",
        "            duration = time.time() - start_time\n",
        "            progress_size = int(count * block_size)\n",
        "            speed = int(progress_size / (1024 * duration))\n",
        "            percent = min(int(count * block_size * 100 / total_size), 100)\n",
        "            sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
        "                             (percent, progress_size / (1024 * 1024), speed, duration))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        urllib.request.urlretrieve(origin, fname, reporthook)\n",
        "        print('Download finished. Start extract!', origin)\n",
        "        if (fname.endswith(\"tar.gz\")):\n",
        "            tar = tarfile.open(fname, \"r:gz\")\n",
        "            tar.extractall()\n",
        "            tar.close()\n",
        "        elif (fname.endswith(\"tar\")):\n",
        "            tar = tarfile.open(fname, \"r:\")\n",
        "            tar.extractall()\n",
        "            tar.close()\n",
        "\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "\n",
        "def load_data_one(file):\n",
        "    batch = unpickle(file)\n",
        "    data = batch[b'data']\n",
        "    labels = batch[b'labels']\n",
        "    print(\"Loading %s : %d.\" % (file, len(data)))\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def load_data(files, data_dir, label_count):\n",
        "    global image_size, img_channels\n",
        "    data, labels = load_data_one(data_dir + '/' + files[0])\n",
        "    for f in files[1:]:\n",
        "        data_n, labels_n = load_data_one(data_dir + '/' + f)\n",
        "        data = np.append(data, data_n, axis=0)\n",
        "        labels = np.append(labels, labels_n, axis=0)\n",
        "    labels = np.array([[float(i == label) for i in range(label_count)] for label in labels])\n",
        "    data = data.reshape([-1, img_channels, image_size, image_size])\n",
        "    data = data.transpose([0, 2, 3, 1])\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def prepare_data():\n",
        "    print(\"======Loading data======\")\n",
        "    download_data()\n",
        "    data_dir = './cifar-10-batches-py'\n",
        "    image_dim = image_size * image_size * img_channels\n",
        "    meta = unpickle(data_dir + '/batches.meta')\n",
        "\n",
        "    label_names = meta[b'label_names']\n",
        "    label_count = len(label_names)\n",
        "    train_files = ['data_batch_%d' % d for d in range(1, 6)]\n",
        "    train_data, train_labels = load_data(train_files, data_dir, label_count)\n",
        "    test_data, test_labels = load_data(['test_batch'], data_dir, label_count)\n",
        "\n",
        "    print(\"Train data:\", np.shape(train_data), np.shape(train_labels))\n",
        "    print(\"Test data :\", np.shape(test_data), np.shape(test_labels))\n",
        "    print(\"======Load finished======\")\n",
        "\n",
        "    print(\"======Shuffling data======\")\n",
        "    indices = np.random.permutation(len(train_data))\n",
        "    train_data = train_data[indices]\n",
        "    train_labels = train_labels[indices]\n",
        "    print(\"======Prepare Finished======\")\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "\n",
        "# ========================================================== #\n",
        "# ├─ _random_crop()\n",
        "# ├─ _random_flip_leftright()\n",
        "# ├─ data_augmentation()\n",
        "# └─ color_preprocessing()\n",
        "# ========================================================== #\n",
        "\n",
        "def _random_crop(batch, crop_shape, padding=None):\n",
        "    oshape = np.shape(batch[0])\n",
        "\n",
        "    if padding:\n",
        "        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n",
        "    new_batch = []\n",
        "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
        "    for i in range(len(batch)):\n",
        "        new_batch.append(batch[i])\n",
        "        if padding:\n",
        "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
        "                                      mode='constant', constant_values=0)\n",
        "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
        "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
        "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
        "                       nw:nw + crop_shape[1]]\n",
        "    return new_batch\n",
        "\n",
        "\n",
        "def _random_flip_leftright(batch):\n",
        "    for i in range(len(batch)):\n",
        "        if bool(random.getrandbits(1)):\n",
        "            batch[i] = np.fliplr(batch[i])\n",
        "    return batch\n",
        "\n",
        "\n",
        "def color_preprocessing(x_train, x_test):\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])\n",
        "    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])\n",
        "    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])\n",
        "\n",
        "    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])\n",
        "    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])\n",
        "    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])\n",
        "\n",
        "    return x_train, x_test\n",
        "\n",
        "\n",
        "def data_augmentation(batch):\n",
        "    batch = _random_flip_leftright(batch)\n",
        "    batch = _random_crop(batch, [32, 32], 4)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXky6v6HOPsi",
        "colab_type": "code",
        "outputId": "78883487-e3f2-4323-85e2-631d45469979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2154
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tflearn.layers.conv import global_avg_pool\n",
        "from tensorflow.contrib.layers import batch_norm, flatten\n",
        "from tensorflow.contrib.framework import arg_scope\n",
        "import numpy as np\n",
        "\n",
        "weight_decay = 0.0005\n",
        "momentum = 0.9\n",
        "\n",
        "init_learning_rate = 0.1\n",
        "cardinality = 8 # how many split ?\n",
        "blocks = 3 # res_block ! (split + transition)\n",
        "depth = 64 # out channel\n",
        "\n",
        "\"\"\"\n",
        "So, the total number of layers is (3*blokcs)*residual_layer_num + 2\n",
        "because, blocks = split(conv 2) + transition(conv 1) = 3 layer\n",
        "and, first conv layer 1, last dense layer 1\n",
        "thus, total number of layers = (3*blocks)*residual_layer_num + 2\n",
        "\"\"\"\n",
        "\n",
        "reduction_ratio = 4\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "iteration = 391\n",
        "# 128 * 391 ~ 50,000\n",
        "\n",
        "test_iteration = 10\n",
        "\n",
        "total_epochs = 100\n",
        "\n",
        "def conv_layer(input, filter, kernel, stride, padding='SAME', layer_name=\"conv\"):\n",
        "    with tf.name_scope(layer_name):\n",
        "        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n",
        "        return network\n",
        "\n",
        "def Global_Average_Pooling(x):\n",
        "    return global_avg_pool(x, name='Global_avg_pooling')\n",
        "\n",
        "def Average_pooling(x, pool_size=[2,2], stride=2, padding='SAME'):\n",
        "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
        "\n",
        "def Batch_Normalization(x, training, scope):\n",
        "    with arg_scope([batch_norm],\n",
        "                   scope=scope,\n",
        "                   updates_collections=None,\n",
        "                   decay=0.9,\n",
        "                   center=True,\n",
        "                   scale=True,\n",
        "                   zero_debias_moving_mean=True) :\n",
        "        return tf.cond(training,\n",
        "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
        "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
        "\n",
        "def Relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "def Sigmoid(x) :\n",
        "    return tf.nn.sigmoid(x)\n",
        "\n",
        "def Concatenation(layers) :\n",
        "    return tf.concat(layers, axis=3)\n",
        "\n",
        "def Fully_connected(x, units=class_num, layer_name='fully_connected') :\n",
        "    with tf.name_scope(layer_name) :\n",
        "        return tf.layers.dense(inputs=x, use_bias=False, units=units)\n",
        "\n",
        "def Evaluate(sess):\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "    test_pre_index = 0\n",
        "    add = 1000\n",
        "\n",
        "    for it in range(test_iteration):\n",
        "        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n",
        "        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n",
        "        test_pre_index = test_pre_index + add\n",
        "\n",
        "        test_feed_dict = {\n",
        "            x: test_batch_x,\n",
        "            label: test_batch_y,\n",
        "            learning_rate: epoch_learning_rate,\n",
        "            training_flag: False\n",
        "        }\n",
        "\n",
        "        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n",
        "\n",
        "        test_loss += loss_\n",
        "        test_acc += acc_\n",
        "\n",
        "    test_loss /= test_iteration # average loss\n",
        "    test_acc /= test_iteration # average accuracy\n",
        "\n",
        "    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=test_loss),\n",
        "                                tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
        "\n",
        "    return test_acc, test_loss, summary\n",
        "\n",
        "class SE_ResNeXt():\n",
        "    def __init__(self, x, training):\n",
        "        self.training = training\n",
        "        self.model = self.Build_SEnet(x)\n",
        "\n",
        "    def first_layer(self, x, scope):\n",
        "        with tf.name_scope(scope) :\n",
        "            x = conv_layer(x, filter=64, kernel=[3, 3], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            x = Relu(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def transform_layer(self, x, stride, scope):\n",
        "        with tf.name_scope(scope) :\n",
        "            x = conv_layer(x, filter=depth, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            x = Relu(x)\n",
        "\n",
        "            x = conv_layer(x, filter=depth, kernel=[3,3], stride=stride, layer_name=scope+'_conv2')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch2')\n",
        "            x = Relu(x)\n",
        "            return x\n",
        "\n",
        "    def transition_layer(self, x, out_dim, scope):\n",
        "        with tf.name_scope(scope):\n",
        "            x = conv_layer(x, filter=out_dim, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            # x = Relu(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def split_layer(self, input_x, stride, layer_name):\n",
        "        with tf.name_scope(layer_name) :\n",
        "            layers_split = list()\n",
        "            for i in range(cardinality) :\n",
        "                splits = self.transform_layer(input_x, stride=stride, scope=layer_name + '_splitN_' + str(i))\n",
        "                layers_split.append(splits)\n",
        "\n",
        "            return Concatenation(layers_split)\n",
        "\n",
        "    def squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n",
        "        with tf.name_scope(layer_name) :\n",
        "\n",
        "\n",
        "            squeeze = Global_Average_Pooling(input_x)\n",
        "\n",
        "            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
        "            excitation = Relu(excitation)\n",
        "            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
        "            excitation = Sigmoid(excitation)\n",
        "\n",
        "            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
        "            scale = input_x * excitation\n",
        "\n",
        "            return scale\n",
        "\n",
        "    def residual_layer(self, input_x, out_dim, layer_num, res_block=blocks):\n",
        "        # split + transform(bottleneck) + transition + merge\n",
        "        # input_dim = input_x.get_shape().as_list()[-1]\n",
        "\n",
        "        for i in range(res_block):\n",
        "            input_dim = int(np.shape(input_x)[-1])\n",
        "\n",
        "            if input_dim * 2 == out_dim:\n",
        "                flag = True\n",
        "                stride = 2\n",
        "                channel = input_dim // 2\n",
        "            else:\n",
        "                flag = False\n",
        "                stride = 1\n",
        "\n",
        "            x = self.split_layer(input_x, stride=stride, layer_name='split_layer_'+layer_num+'_'+str(i))\n",
        "            x = self.transition_layer(x, out_dim=out_dim, scope='trans_layer_'+layer_num+'_'+str(i))\n",
        "            x = self.squeeze_excitation_layer(x, out_dim=out_dim, ratio=reduction_ratio, layer_name='squeeze_layer_'+layer_num+'_'+str(i))\n",
        "\n",
        "            if flag is True :\n",
        "                pad_input_x = Average_pooling(input_x)\n",
        "                pad_input_x = tf.pad(pad_input_x, [[0, 0], [0, 0], [0, 0], [channel, channel]]) # [?, height, width, channel]\n",
        "            else :\n",
        "                pad_input_x = input_x\n",
        "\n",
        "            input_x = Relu(x + pad_input_x)\n",
        "\n",
        "        return input_x\n",
        "\n",
        "\n",
        "    def Build_SEnet(self, input_x):\n",
        "        # only cifar10 architecture\n",
        "\n",
        "        input_x = self.first_layer(input_x, scope='first_layer')\n",
        "\n",
        "        x = self.residual_layer(input_x, out_dim=64, layer_num='1')\n",
        "        x = self.residual_layer(x, out_dim=128, layer_num='2')\n",
        "        x = self.residual_layer(x, out_dim=256, layer_num='3')\n",
        "\n",
        "        x = Global_Average_Pooling(x)\n",
        "        x = flatten(x)\n",
        "\n",
        "        x = Fully_connected(x, layer_name='final_fully_connected')\n",
        "        return x\n",
        "\n",
        "\n",
        "train_x, train_y, test_x, test_y = prepare_data()\n",
        "train_x, test_x = color_preprocessing(train_x, test_x)\n",
        "\n",
        "\n",
        "# image_size = 32, img_channels = 3, class_num = 10 in cifar10\n",
        "x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\n",
        "label = tf.placeholder(tf.float32, shape=[None, class_num])\n",
        "\n",
        "training_flag = tf.placeholder(tf.bool)\n",
        "\n",
        "\n",
        "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
        "\n",
        "logits = SE_ResNeXt(x, training=training_flag).model\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))\n",
        "\n",
        "l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
        "train = optimizer.minimize(cost + l2_loss * weight_decay)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "with tf.Session() as sess:\n",
        "# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess :\n",
        "    ckpt = tf.train.get_checkpoint_state('./model')\n",
        "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
        "\n",
        "    epoch_learning_rate = init_learning_rate\n",
        "    for epoch in range(1, total_epochs + 1):\n",
        "        if epoch % 30 == 0 :\n",
        "            epoch_learning_rate = epoch_learning_rate / 10\n",
        "\n",
        "        pre_index = 0\n",
        "        train_acc = 0.0\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for step in range(1, iteration + 1):\n",
        "            if pre_index + batch_size < 50000:\n",
        "                batch_x = train_x[pre_index: pre_index + batch_size]\n",
        "                batch_y = train_y[pre_index: pre_index + batch_size]\n",
        "            else:\n",
        "                batch_x = train_x[pre_index:]\n",
        "                batch_y = train_y[pre_index:]\n",
        "\n",
        "            batch_x = data_augmentation(batch_x)\n",
        "\n",
        "            train_feed_dict = {\n",
        "                x: batch_x,\n",
        "                label: batch_y,\n",
        "                learning_rate: epoch_learning_rate,\n",
        "                training_flag: True\n",
        "            }\n",
        "\n",
        "            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n",
        "            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n",
        "\n",
        "            train_loss += batch_loss\n",
        "            train_acc += batch_acc\n",
        "            pre_index += batch_size\n",
        "\n",
        "\n",
        "        train_loss /= iteration # average loss\n",
        "        train_acc /= iteration # average accuracy\n",
        "\n",
        "        train_summary = tf.Summary(value=[tf.Summary.Value(tag='train_loss', simple_value=train_loss),\n",
        "                                          tf.Summary.Value(tag='train_accuracy', simple_value=train_acc)])\n",
        "\n",
        "        test_acc, test_loss, test_summary = Evaluate(sess)\n",
        "\n",
        "        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n",
        "        summary_writer.add_summary(summary=test_summary, global_step=epoch)\n",
        "        summary_writer.flush()\n",
        "\n",
        "        line = \"epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n\" % (\n",
        "            epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n",
        "        print(line)\n",
        "\n",
        "        with open('logs.txt', 'a') as f:\n",
        "            f.write(line)\n",
        "\n",
        "        saver.save(sess=sess, save_path='./model/ResNeXt.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "======Loading data======\n",
            "Downloading data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "...100%, 162 MB, 6885 KB/s, 24 seconds passedDownload finished. Start extract! http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Loading ./cifar-10-batches-py/data_batch_1 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_2 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_3 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_4 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_5 : 10000.\n",
            "Loading ./cifar-10-batches-py/test_batch : 10000.\n",
            "Train data: (50000, 32, 32, 3) (50000, 10)\n",
            "Test data : (10000, 32, 32, 3) (10000, 10)\n",
            "======Load finished======\n",
            "======Shuffling data======\n",
            "======Prepare Finished======\n",
            "WARNING:tensorflow:From <ipython-input-2-4764bd4f8c0e>:35: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-4764bd4f8c0e>:67: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-4764bd4f8c0e>:42: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.average_pooling2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-4764bd4f8c0e>:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "epoch: 1/100, train_loss: 1.4760, train_acc: 0.5630, test_loss: 1.2205, test_acc: 0.5725 \n",
            "\n",
            "epoch: 2/100, train_loss: 0.8855, train_acc: 0.7815, test_loss: 0.8191, test_acc: 0.7152 \n",
            "\n",
            "epoch: 3/100, train_loss: 0.6660, train_acc: 0.8621, test_loss: 0.7299, test_acc: 0.7395 \n",
            "\n",
            "epoch: 4/100, train_loss: 0.5633, train_acc: 0.8980, test_loss: 0.5939, test_acc: 0.7952 \n",
            "\n",
            "epoch: 5/100, train_loss: 0.5054, train_acc: 0.9188, test_loss: 0.5235, test_acc: 0.8228 \n",
            "\n",
            "epoch: 6/100, train_loss: 0.4681, train_acc: 0.9339, test_loss: 0.5654, test_acc: 0.8065 \n",
            "\n",
            "epoch: 7/100, train_loss: 0.4441, train_acc: 0.9407, test_loss: 0.4804, test_acc: 0.8353 \n",
            "\n",
            "epoch: 8/100, train_loss: 0.4201, train_acc: 0.9506, test_loss: 0.5070, test_acc: 0.8227 \n",
            "\n",
            "epoch: 9/100, train_loss: 0.4098, train_acc: 0.9541, test_loss: 0.5732, test_acc: 0.8083 \n",
            "\n",
            "epoch: 10/100, train_loss: 0.3923, train_acc: 0.9613, test_loss: 0.5755, test_acc: 0.8069 \n",
            "\n",
            "epoch: 11/100, train_loss: 0.3861, train_acc: 0.9618, test_loss: 0.4872, test_acc: 0.8331 \n",
            "\n",
            "epoch: 12/100, train_loss: 0.3744, train_acc: 0.9663, test_loss: 0.5475, test_acc: 0.8190 \n",
            "\n",
            "epoch: 13/100, train_loss: 0.3654, train_acc: 0.9677, test_loss: 0.4923, test_acc: 0.8333 \n",
            "\n",
            "epoch: 14/100, train_loss: 0.3589, train_acc: 0.9694, test_loss: 0.5400, test_acc: 0.8209 \n",
            "\n",
            "epoch: 15/100, train_loss: 0.3481, train_acc: 0.9711, test_loss: 0.4802, test_acc: 0.8397 \n",
            "\n",
            "epoch: 16/100, train_loss: 0.3429, train_acc: 0.9725, test_loss: 0.4164, test_acc: 0.8595 \n",
            "\n",
            "epoch: 17/100, train_loss: 0.3356, train_acc: 0.9739, test_loss: 0.4907, test_acc: 0.8348 \n",
            "\n",
            "epoch: 18/100, train_loss: 0.3308, train_acc: 0.9757, test_loss: 0.5855, test_acc: 0.8114 \n",
            "\n",
            "epoch: 19/100, train_loss: 0.3259, train_acc: 0.9762, test_loss: 0.3923, test_acc: 0.8663 \n",
            "\n",
            "epoch: 20/100, train_loss: 0.3242, train_acc: 0.9771, test_loss: 0.4446, test_acc: 0.8529 \n",
            "\n",
            "epoch: 21/100, train_loss: 0.3187, train_acc: 0.9786, test_loss: 0.5188, test_acc: 0.8315 \n",
            "\n",
            "epoch: 22/100, train_loss: 0.3153, train_acc: 0.9787, test_loss: 0.4508, test_acc: 0.8501 \n",
            "\n",
            "epoch: 23/100, train_loss: 0.3090, train_acc: 0.9801, test_loss: 0.4806, test_acc: 0.8416 \n",
            "\n",
            "epoch: 24/100, train_loss: 0.3068, train_acc: 0.9809, test_loss: 0.4585, test_acc: 0.8507 \n",
            "\n",
            "epoch: 25/100, train_loss: 0.3062, train_acc: 0.9800, test_loss: 0.4433, test_acc: 0.8536 \n",
            "\n",
            "epoch: 26/100, train_loss: 0.3028, train_acc: 0.9818, test_loss: 0.4501, test_acc: 0.8521 \n",
            "\n",
            "epoch: 27/100, train_loss: 0.2992, train_acc: 0.9812, test_loss: 0.4904, test_acc: 0.8337 \n",
            "\n",
            "epoch: 28/100, train_loss: 0.2970, train_acc: 0.9821, test_loss: 0.4137, test_acc: 0.8605 \n",
            "\n",
            "epoch: 29/100, train_loss: 0.2972, train_acc: 0.9819, test_loss: 0.4393, test_acc: 0.8552 \n",
            "\n",
            "epoch: 30/100, train_loss: 0.1628, train_acc: 0.9543, test_loss: 0.2138, test_acc: 0.9240 \n",
            "\n",
            "epoch: 31/100, train_loss: 0.1200, train_acc: 0.9697, test_loss: 0.2034, test_acc: 0.9304 \n",
            "\n",
            "epoch: 32/100, train_loss: 0.1002, train_acc: 0.9765, test_loss: 0.1974, test_acc: 0.9332 \n",
            "\n",
            "epoch: 33/100, train_loss: 0.0878, train_acc: 0.9819, test_loss: 0.2032, test_acc: 0.9329 \n",
            "\n",
            "epoch: 34/100, train_loss: 0.0784, train_acc: 0.9852, test_loss: 0.1989, test_acc: 0.9340 \n",
            "\n",
            "epoch: 35/100, train_loss: 0.0685, train_acc: 0.9893, test_loss: 0.1985, test_acc: 0.9340 \n",
            "\n",
            "epoch: 36/100, train_loss: 0.0608, train_acc: 0.9921, test_loss: 0.1959, test_acc: 0.9360 \n",
            "\n",
            "epoch: 37/100, train_loss: 0.0543, train_acc: 0.9945, test_loss: 0.1933, test_acc: 0.9372 \n",
            "\n",
            "epoch: 38/100, train_loss: 0.0480, train_acc: 0.9960, test_loss: 0.2071, test_acc: 0.9326 \n",
            "\n",
            "epoch: 39/100, train_loss: 0.0430, train_acc: 0.9972, test_loss: 0.2088, test_acc: 0.9335 \n",
            "\n",
            "epoch: 40/100, train_loss: 0.0399, train_acc: 0.9980, test_loss: 0.2073, test_acc: 0.9358 \n",
            "\n",
            "epoch: 41/100, train_loss: 0.0368, train_acc: 0.9986, test_loss: 0.2083, test_acc: 0.9336 \n",
            "\n",
            "epoch: 42/100, train_loss: 0.0342, train_acc: 0.9990, test_loss: 0.2130, test_acc: 0.9338 \n",
            "\n",
            "epoch: 43/100, train_loss: 0.0310, train_acc: 0.9996, test_loss: 0.2207, test_acc: 0.9315 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDck_5l30R1-",
        "colab_type": "text"
      },
      "source": [
        "The following cells are written or modified by TonyNg, published on https://github.com/TonyNgcn/meme-classification \n",
        "You can download my model on https://drive.google.com/file/d/1zDwXJxuLPkODAG99Yvd7VLCqihPTKkUS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNlEiXgKFCMJ",
        "colab_type": "code",
        "outputId": "50f9ee48-caa2-4ac4-b9f9-39e7c543f3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "!wget https://github.com/TonyNgcn/meme-classification/raw/master/ok.zip\n",
        "!unzip ok.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-14 03:01:00--  https://github.com/TonyNgcn/meme-classification/raw/master/ok.zip\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/TonyNgcn/meme-classification/master/ok.zip [following]\n",
            "--2019-06-14 03:01:00--  https://raw.githubusercontent.com/TonyNgcn/meme-classification/master/ok.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5510451 (5.3M) [application/zip]\n",
            "Saving to: ‘ok.zip’\n",
            "\n",
            "\rok.zip                0%[                    ]       0  --.-KB/s               \rok.zip              100%[===================>]   5.25M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-06-14 03:01:00 (55.2 MB/s) - ‘ok.zip’ saved [5510451/5510451]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kQS9mqEE9Fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import os\n",
        "import random\n",
        "class_num = 10\n",
        "image_size = 32\n",
        "img_channels = 3\n",
        "\n",
        "\n",
        "def get_rgb_vector(imgPath):\n",
        "\timg = cv.imread(imgPath)\n",
        "\t# img=cv.resize(img,(32,32))\n",
        "\tB, G, R = cv.split(img)\n",
        "\tB = np.reshape(B, (1, 1024))\n",
        "\tG = np.reshape(G, (1, 1024))\n",
        "\tR = np.reshape(R, (1, 1024))\n",
        "\tvector = np.concatenate((R, G, B), axis=1)\n",
        "\treturn vector\n",
        "\n",
        "\n",
        "def prepare_data():\n",
        "\tdir='./ok'\n",
        "\tlabels=[]\n",
        "\tdata=[]\n",
        "\tfor i,imgpath in enumerate(os.listdir(dir)):\n",
        "\t\t# if i==0:\n",
        "\t\t# \tdata=get_rgb_vector(dir+'/'+imgpath)\n",
        "\t\t# else:\n",
        "\t\t# \tdata = np.concatenate((data,get_rgb_vector(dir+'/'+imgpath)), axis=0)\n",
        "\n",
        "\t\tdata.append(cv.imread(dir+'/'+imgpath))\n",
        "\t\tlabels.append(int(imgpath[:2]))\n",
        "\tlabels=np.array(labels)\n",
        "\tdata=np.array(data)\n",
        "\n",
        "\ttrain_data, test_data, train_labels, test_labels = train_test_split(data,labels,test_size=0.2)\n",
        "\n",
        "\tonehot=OneHotEncoder()\n",
        "\ttrain_labels=onehot.fit_transform(train_labels.reshape(-1,1)).toarray()\n",
        "\ttest_labels=onehot.fit_transform(test_labels.reshape(-1,1)).toarray()\n",
        "\n",
        "\treturn train_data, train_labels, test_data, test_labels\n",
        "\n",
        "\n",
        "def _random_crop(batch, crop_shape, padding=None):\n",
        "    oshape = np.shape(batch[0])\n",
        "\n",
        "    if padding:\n",
        "        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n",
        "    new_batch = []\n",
        "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
        "    for i in range(len(batch)):\n",
        "        new_batch.append(batch[i])\n",
        "        if padding:\n",
        "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
        "                                      mode='constant', constant_values=0)\n",
        "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
        "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
        "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
        "                       nw:nw + crop_shape[1]]\n",
        "    return new_batch\n",
        "\n",
        "\n",
        "def _random_flip_leftright(batch):\n",
        "    for i in range(len(batch)):\n",
        "        if bool(random.getrandbits(1)):\n",
        "            batch[i] = np.fliplr(batch[i])\n",
        "    return batch\n",
        "\n",
        "\n",
        "def color_preprocessing(x_train, x_test):\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])\n",
        "    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])\n",
        "    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])\n",
        "\n",
        "    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])\n",
        "    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])\n",
        "    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])\n",
        "\n",
        "    return x_train, x_test\n",
        "\n",
        "\n",
        "def data_augmentation(batch):\n",
        "    batch = _random_flip_leftright(batch)\n",
        "    batch = _random_crop(batch, [32, 32], 4)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UWLogknL0hI",
        "colab_type": "text"
      },
      "source": [
        "The following cell is modified from SE_ResNeXt.py in order to fit the dataset by myself.(The meme picture)-v1.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEP7VSfgFO9n",
        "colab_type": "code",
        "outputId": "10ec4b23-8e01-4871-e5d3-d2e57025e2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3896
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tflearn.layers.conv import global_avg_pool\n",
        "from tensorflow.contrib.layers import batch_norm, flatten\n",
        "from tensorflow.contrib.framework import arg_scope\n",
        "import keras.layers\n",
        "import numpy as np\n",
        "\n",
        "weight_decay = 0.0005\n",
        "momentum = 0.9\n",
        "\n",
        "init_learning_rate = 0.1\n",
        "cardinality = 8 # how many split ?\n",
        "blocks = 3 # res_block ! (split + transition)\n",
        "depth = 64 # out channel\n",
        "\n",
        "\"\"\"\n",
        "So, the total number of layers is (3*blokcs)*residual_layer_num + 2\n",
        "because, blocks = split(conv 2) + transition(conv 1) = 3 layer\n",
        "and, first conv layer 1, last dense layer 1\n",
        "thus, total number of layers = (3*blocks)*residual_layer_num + 2\n",
        "\"\"\"\n",
        "\n",
        "reduction_ratio = 4\n",
        "class_num=10\n",
        "batch_size = 128\n",
        "\n",
        "# iteration = 391\n",
        "# 128 * 391 ~ 50,000\n",
        "\n",
        "iteration=36\n",
        "# 128*36~4608\n",
        "\n",
        "test_iteration = 10\n",
        "\n",
        "total_epochs = 100\n",
        "\n",
        "def conv_layer(input, filter, kernel, stride, padding='SAME', layer_name=\"conv\"):\n",
        "    with tf.name_scope(layer_name):\n",
        "        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n",
        "        return network\n",
        "\n",
        "def Global_Average_Pooling(x):\n",
        "    return global_avg_pool(x, name='Global_avg_pooling')\n",
        "\n",
        "def Average_pooling(x, pool_size=[2,2], stride=2, padding='SAME'):\n",
        "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
        "\n",
        "def Batch_Normalization(x, training, scope):\n",
        "    with arg_scope([batch_norm],\n",
        "                   scope=scope,\n",
        "                   updates_collections=None,\n",
        "                   decay=0.9,\n",
        "                   center=True,\n",
        "                   scale=True,\n",
        "                   zero_debias_moving_mean=True) :\n",
        "        return tf.cond(training,\n",
        "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
        "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
        "\n",
        "def Relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "def Sigmoid(x) :\n",
        "    return tf.nn.sigmoid(x)\n",
        "\n",
        "def Concatenation(layers) :\n",
        "    return tf.concat(layers, axis=3)\n",
        "\n",
        "def Fully_connected(x, units=class_num, layer_name='fully_connected') :\n",
        "    with tf.name_scope(layer_name) :\n",
        "        return tf.layers.dense(inputs=x, use_bias=False, units=units)\n",
        "\n",
        "def Evaluate(sess):\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "    test_pre_index = 0\n",
        "    add = int(len(test_x)/test_iteration)\n",
        "\n",
        "    for it in range(test_iteration):\n",
        "        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n",
        "        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n",
        "        test_pre_index = test_pre_index + add\n",
        "\n",
        "        test_feed_dict = {\n",
        "            x: test_batch_x,\n",
        "            label: test_batch_y,\n",
        "            learning_rate: epoch_learning_rate,\n",
        "            training_flag: False\n",
        "        }\n",
        "\n",
        "        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n",
        "\n",
        "        test_loss += loss_\n",
        "        test_acc += acc_\n",
        "\n",
        "    test_loss /= test_iteration # average loss\n",
        "    test_acc /= test_iteration # average accuracy\n",
        "\n",
        "    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=test_loss),\n",
        "                                tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
        "\n",
        "    return test_acc, test_loss, summary\n",
        "\n",
        "class SE_ResNeXt():\n",
        "    def __init__(self, x, training):\n",
        "        self.training = training\n",
        "        self.model = self.Build_SEnet(x)\n",
        "\n",
        "    def first_layer(self, x, scope):\n",
        "        with tf.name_scope(scope) :\n",
        "            x = conv_layer(x, filter=64, kernel=[3, 3], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            x = Relu(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def transform_layer(self, x, stride, scope):\n",
        "        with tf.name_scope(scope) :\n",
        "            x = conv_layer(x, filter=depth, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            x = Relu(x)\n",
        "\n",
        "            x = conv_layer(x, filter=depth, kernel=[3,3], stride=stride, layer_name=scope+'_conv2')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch2')\n",
        "            x = Relu(x)\n",
        "            return x\n",
        "\n",
        "    def transition_layer(self, x, out_dim, scope):\n",
        "        with tf.name_scope(scope):\n",
        "            x = conv_layer(x, filter=out_dim, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            # x = Relu(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def split_layer(self, input_x, stride, layer_name):\n",
        "        with tf.name_scope(layer_name) :\n",
        "            layers_split = list()\n",
        "            for i in range(cardinality) :\n",
        "                splits = self.transform_layer(input_x, stride=stride, scope=layer_name + '_splitN_' + str(i))\n",
        "                layers_split.append(splits)\n",
        "\n",
        "            return Concatenation(layers_split)\n",
        "\n",
        "    def squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n",
        "        with tf.name_scope(layer_name) :\n",
        "\n",
        "\n",
        "            squeeze = Global_Average_Pooling(input_x)\n",
        "\n",
        "            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
        "            excitation = Relu(excitation)\n",
        "            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
        "            excitation = Sigmoid(excitation)\n",
        "\n",
        "            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
        "            scale = input_x * excitation\n",
        "\n",
        "            return scale\n",
        "\n",
        "    def residual_layer(self, input_x, out_dim, layer_num, res_block=blocks):\n",
        "        # split + transform(bottleneck) + transition + merge\n",
        "        # input_dim = input_x.get_shape().as_list()[-1]\n",
        "\n",
        "        for i in range(res_block):\n",
        "            input_dim = int(np.shape(input_x)[-1])\n",
        "\n",
        "            if input_dim * 2 == out_dim:\n",
        "                flag = True\n",
        "                stride = 2\n",
        "                channel = input_dim // 2\n",
        "            else:\n",
        "                flag = False\n",
        "                stride = 1\n",
        "\n",
        "            x = self.split_layer(input_x, stride=stride, layer_name='split_layer_'+layer_num+'_'+str(i))\n",
        "            x = self.transition_layer(x, out_dim=out_dim, scope='trans_layer_'+layer_num+'_'+str(i))\n",
        "            x = self.squeeze_excitation_layer(x, out_dim=out_dim, ratio=reduction_ratio, layer_name='squeeze_layer_'+layer_num+'_'+str(i))\n",
        "\n",
        "            if flag is True :\n",
        "                pad_input_x = Average_pooling(input_x)\n",
        "                pad_input_x = tf.pad(pad_input_x, [[0, 0], [0, 0], [0, 0], [channel, channel]]) # [?, height, width, channel]\n",
        "            else :\n",
        "                pad_input_x = input_x\n",
        "\n",
        "            input_x = Relu(x + pad_input_x)\n",
        "\n",
        "        return input_x\n",
        "\n",
        "\n",
        "    def Build_SEnet(self, input_x):\n",
        "        # only cifar10 architecture\n",
        "\n",
        "        input_x = self.first_layer(input_x, scope='first_layer')\n",
        "\n",
        "        x = self.residual_layer(input_x, out_dim=64, layer_num='1')\n",
        "        x = self.residual_layer(x, out_dim=128, layer_num='2')\n",
        "        x = self.residual_layer(x, out_dim=256, layer_num='3')\n",
        "\n",
        "        x = Global_Average_Pooling(x)\n",
        "        x = flatten(x)\n",
        "\n",
        "        x = Fully_connected(x, layer_name='final_fully_connected')\n",
        "        return x\n",
        "\n",
        "\n",
        "train_x, train_y, test_x, test_y = prepare_data()\n",
        "train_x, test_x = color_preprocessing(train_x, test_x)\n",
        "\n",
        "\n",
        "# image_size = 32, img_channels = 3, class_num = 10 in cifar10\n",
        "x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\n",
        "label = tf.placeholder(tf.float32, shape=[None, class_num])\n",
        "\n",
        "training_flag = tf.placeholder(tf.bool)\n",
        "\n",
        "\n",
        "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
        "\n",
        "logits = SE_ResNeXt(x, training=training_flag).model\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logits))\n",
        "\n",
        "l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
        "train = optimizer.minimize(cost + l2_loss * weight_decay)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess :\n",
        "    ckpt = tf.train.get_checkpoint_state('./model')\n",
        "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
        "\n",
        "    epoch_learning_rate = init_learning_rate\n",
        "    for epoch in range(1, total_epochs + 1):\n",
        "        if epoch % 30 == 0 :\n",
        "            epoch_learning_rate = epoch_learning_rate / 10\n",
        "\n",
        "        pre_index = 0\n",
        "        train_acc = 0.0\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for step in range(1, iteration + 1):\n",
        "            if pre_index + batch_size < 4586:\n",
        "                batch_x = train_x[pre_index: pre_index + batch_size]\n",
        "                batch_y = train_y[pre_index: pre_index + batch_size]\n",
        "            else:\n",
        "                batch_x = train_x[pre_index:]\n",
        "                batch_y = train_y[pre_index:]\n",
        "\n",
        "            batch_x = data_augmentation(batch_x)\n",
        "\n",
        "            train_feed_dict = {\n",
        "                x: batch_x,\n",
        "                label: batch_y,\n",
        "                learning_rate: epoch_learning_rate,\n",
        "                training_flag: True\n",
        "            }\n",
        "\n",
        "            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n",
        "            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n",
        "\n",
        "            train_loss += batch_loss\n",
        "            train_acc += batch_acc\n",
        "            pre_index += batch_size\n",
        "\n",
        "\n",
        "        train_loss /= iteration # average loss\n",
        "        train_acc /= iteration # average accuracy\n",
        "\n",
        "        train_summary = tf.Summary(value=[tf.Summary.Value(tag='train_loss', simple_value=train_loss),\n",
        "                                          tf.Summary.Value(tag='train_accuracy', simple_value=train_acc)])\n",
        "\n",
        "        test_acc, test_loss, test_summary = Evaluate(sess)\n",
        "\n",
        "        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n",
        "        summary_writer.add_summary(summary=test_summary, global_step=epoch)\n",
        "        summary_writer.flush()\n",
        "\n",
        "        line = \"epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n\" % (\n",
        "            epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n",
        "        print(line)\n",
        "\n",
        "        with open('logs.txt', 'a') as f:\n",
        "            f.write(line)\n",
        "\n",
        "        saver.save(sess=sess, save_path='./model/ResNeXt.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-140018fdab69>:39: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-140018fdab69>:71: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-140018fdab69>:46: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.average_pooling2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "epoch: 1/100, train_loss: 1.5237, train_acc: 0.5992, test_loss: 2.5079, test_acc: 0.4474 \n",
            "\n",
            "epoch: 2/100, train_loss: 0.7301, train_acc: 0.7969, test_loss: 0.9444, test_acc: 0.6298 \n",
            "\n",
            "epoch: 3/100, train_loss: 0.5545, train_acc: 0.8560, test_loss: 0.8066, test_acc: 0.7211 \n",
            "\n",
            "epoch: 4/100, train_loss: 0.4585, train_acc: 0.8890, test_loss: 2.7603, test_acc: 0.6351 \n",
            "\n",
            "epoch: 5/100, train_loss: 0.3814, train_acc: 0.9119, test_loss: 0.6498, test_acc: 0.7798 \n",
            "\n",
            "epoch: 6/100, train_loss: 0.3392, train_acc: 0.9330, test_loss: 0.6694, test_acc: 0.7640 \n",
            "\n",
            "epoch: 7/100, train_loss: 0.2879, train_acc: 0.9489, test_loss: 0.5575, test_acc: 0.8123 \n",
            "\n",
            "epoch: 8/100, train_loss: 0.2560, train_acc: 0.9570, test_loss: 0.5260, test_acc: 0.8114 \n",
            "\n",
            "epoch: 9/100, train_loss: 0.2325, train_acc: 0.9624, test_loss: 1.8418, test_acc: 0.6605 \n",
            "\n",
            "epoch: 10/100, train_loss: 0.1993, train_acc: 0.9723, test_loss: 0.3894, test_acc: 0.8684 \n",
            "\n",
            "epoch: 11/100, train_loss: 0.1841, train_acc: 0.9831, test_loss: 0.5343, test_acc: 0.8333 \n",
            "\n",
            "epoch: 12/100, train_loss: 0.1731, train_acc: 0.9796, test_loss: 1.7052, test_acc: 0.7386 \n",
            "\n",
            "epoch: 13/100, train_loss: 0.1456, train_acc: 0.9882, test_loss: 1.0554, test_acc: 0.7500 \n",
            "\n",
            "epoch: 14/100, train_loss: 0.1394, train_acc: 0.9887, test_loss: 1.7567, test_acc: 0.7719 \n",
            "\n",
            "epoch: 15/100, train_loss: 0.1197, train_acc: 0.9922, test_loss: 2.5930, test_acc: 0.6912 \n",
            "\n",
            "epoch: 16/100, train_loss: 0.1217, train_acc: 0.9932, test_loss: 2.9808, test_acc: 0.6860 \n",
            "\n",
            "epoch: 17/100, train_loss: 0.1022, train_acc: 0.9941, test_loss: 1.4257, test_acc: 0.7632 \n",
            "\n",
            "epoch: 18/100, train_loss: 0.1140, train_acc: 0.9937, test_loss: 0.5178, test_acc: 0.8289 \n",
            "\n",
            "epoch: 19/100, train_loss: 0.0943, train_acc: 0.9973, test_loss: 0.3266, test_acc: 0.9044 \n",
            "\n",
            "epoch: 20/100, train_loss: 0.0913, train_acc: 0.9976, test_loss: 0.3009, test_acc: 0.8965 \n",
            "\n",
            "epoch: 21/100, train_loss: 0.0897, train_acc: 0.9972, test_loss: 0.6519, test_acc: 0.8289 \n",
            "\n",
            "epoch: 22/100, train_loss: 0.0944, train_acc: 0.9954, test_loss: 0.2792, test_acc: 0.9132 \n",
            "\n",
            "epoch: 23/100, train_loss: 0.0757, train_acc: 0.9976, test_loss: 0.4880, test_acc: 0.8649 \n",
            "\n",
            "epoch: 24/100, train_loss: 0.0911, train_acc: 0.9965, test_loss: 0.6155, test_acc: 0.8237 \n",
            "\n",
            "epoch: 25/100, train_loss: 0.0724, train_acc: 0.9993, test_loss: 2.8552, test_acc: 0.7044 \n",
            "\n",
            "epoch: 26/100, train_loss: 0.0849, train_acc: 0.9976, test_loss: 0.2870, test_acc: 0.9132 \n",
            "\n",
            "epoch: 27/100, train_loss: 0.0630, train_acc: 0.9993, test_loss: 0.2192, test_acc: 0.9412 \n",
            "\n",
            "epoch: 28/100, train_loss: 0.0699, train_acc: 0.9983, test_loss: 0.5575, test_acc: 0.8430 \n",
            "\n",
            "epoch: 29/100, train_loss: 0.0872, train_acc: 0.9967, test_loss: 0.3426, test_acc: 0.8930 \n",
            "\n",
            "epoch: 30/100, train_loss: 0.0382, train_acc: 0.9928, test_loss: 0.1254, test_acc: 0.9623 \n",
            "\n",
            "epoch: 31/100, train_loss: 0.0208, train_acc: 0.9974, test_loss: 0.1080, test_acc: 0.9711 \n",
            "\n",
            "epoch: 32/100, train_loss: 0.0152, train_acc: 0.9980, test_loss: 0.1001, test_acc: 0.9728 \n",
            "\n",
            "epoch: 33/100, train_loss: 0.0108, train_acc: 0.9996, test_loss: 0.0985, test_acc: 0.9737 \n",
            "\n",
            "epoch: 34/100, train_loss: 0.0102, train_acc: 0.9996, test_loss: 0.0937, test_acc: 0.9728 \n",
            "\n",
            "epoch: 35/100, train_loss: 0.0089, train_acc: 0.9996, test_loss: 0.0900, test_acc: 0.9737 \n",
            "\n",
            "epoch: 36/100, train_loss: 0.0081, train_acc: 0.9998, test_loss: 0.0906, test_acc: 0.9772 \n",
            "\n",
            "epoch: 37/100, train_loss: 0.0070, train_acc: 1.0000, test_loss: 0.0896, test_acc: 0.9772 \n",
            "\n",
            "epoch: 38/100, train_loss: 0.0072, train_acc: 0.9996, test_loss: 0.0909, test_acc: 0.9789 \n",
            "\n",
            "epoch: 39/100, train_loss: 0.0061, train_acc: 0.9996, test_loss: 0.0890, test_acc: 0.9763 \n",
            "\n",
            "epoch: 40/100, train_loss: 0.0061, train_acc: 1.0000, test_loss: 0.0867, test_acc: 0.9763 \n",
            "\n",
            "epoch: 41/100, train_loss: 0.0065, train_acc: 0.9996, test_loss: 0.0868, test_acc: 0.9781 \n",
            "\n",
            "epoch: 42/100, train_loss: 0.0058, train_acc: 0.9998, test_loss: 0.0851, test_acc: 0.9754 \n",
            "\n",
            "epoch: 43/100, train_loss: 0.0058, train_acc: 0.9996, test_loss: 0.0833, test_acc: 0.9816 \n",
            "\n",
            "epoch: 44/100, train_loss: 0.0049, train_acc: 1.0000, test_loss: 0.0839, test_acc: 0.9781 \n",
            "\n",
            "epoch: 45/100, train_loss: 0.0047, train_acc: 1.0000, test_loss: 0.0838, test_acc: 0.9789 \n",
            "\n",
            "epoch: 46/100, train_loss: 0.0049, train_acc: 1.0000, test_loss: 0.0829, test_acc: 0.9789 \n",
            "\n",
            "epoch: 47/100, train_loss: 0.0043, train_acc: 1.0000, test_loss: 0.0854, test_acc: 0.9789 \n",
            "\n",
            "epoch: 48/100, train_loss: 0.0045, train_acc: 1.0000, test_loss: 0.0836, test_acc: 0.9816 \n",
            "\n",
            "epoch: 49/100, train_loss: 0.0043, train_acc: 1.0000, test_loss: 0.0830, test_acc: 0.9807 \n",
            "\n",
            "epoch: 50/100, train_loss: 0.0042, train_acc: 1.0000, test_loss: 0.0809, test_acc: 0.9816 \n",
            "\n",
            "epoch: 51/100, train_loss: 0.0042, train_acc: 1.0000, test_loss: 0.0794, test_acc: 0.9789 \n",
            "\n",
            "epoch: 52/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0806, test_acc: 0.9807 \n",
            "\n",
            "epoch: 53/100, train_loss: 0.0040, train_acc: 1.0000, test_loss: 0.0799, test_acc: 0.9798 \n",
            "\n",
            "epoch: 54/100, train_loss: 0.0040, train_acc: 1.0000, test_loss: 0.0808, test_acc: 0.9789 \n",
            "\n",
            "epoch: 55/100, train_loss: 0.0039, train_acc: 1.0000, test_loss: 0.0784, test_acc: 0.9798 \n",
            "\n",
            "epoch: 56/100, train_loss: 0.0039, train_acc: 1.0000, test_loss: 0.0786, test_acc: 0.9816 \n",
            "\n",
            "epoch: 57/100, train_loss: 0.0040, train_acc: 1.0000, test_loss: 0.0786, test_acc: 0.9807 \n",
            "\n",
            "epoch: 58/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0781, test_acc: 0.9798 \n",
            "\n",
            "epoch: 59/100, train_loss: 0.0037, train_acc: 1.0000, test_loss: 0.0757, test_acc: 0.9781 \n",
            "\n",
            "epoch: 60/100, train_loss: 0.0036, train_acc: 1.0000, test_loss: 0.0769, test_acc: 0.9781 \n",
            "\n",
            "epoch: 61/100, train_loss: 0.0036, train_acc: 1.0000, test_loss: 0.0758, test_acc: 0.9789 \n",
            "\n",
            "epoch: 62/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0773, test_acc: 0.9781 \n",
            "\n",
            "epoch: 63/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0770, test_acc: 0.9798 \n",
            "\n",
            "epoch: 64/100, train_loss: 0.0035, train_acc: 0.9998, test_loss: 0.0770, test_acc: 0.9772 \n",
            "\n",
            "epoch: 65/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0766, test_acc: 0.9798 \n",
            "\n",
            "epoch: 66/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0757, test_acc: 0.9798 \n",
            "\n",
            "epoch: 67/100, train_loss: 0.0038, train_acc: 1.0000, test_loss: 0.0760, test_acc: 0.9798 \n",
            "\n",
            "epoch: 68/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0765, test_acc: 0.9798 \n",
            "\n",
            "epoch: 69/100, train_loss: 0.0037, train_acc: 0.9998, test_loss: 0.0765, test_acc: 0.9789 \n",
            "\n",
            "epoch: 70/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0758, test_acc: 0.9798 \n",
            "\n",
            "epoch: 71/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0773, test_acc: 0.9807 \n",
            "\n",
            "epoch: 72/100, train_loss: 0.0037, train_acc: 1.0000, test_loss: 0.0760, test_acc: 0.9798 \n",
            "\n",
            "epoch: 73/100, train_loss: 0.0038, train_acc: 0.9998, test_loss: 0.0766, test_acc: 0.9816 \n",
            "\n",
            "epoch: 74/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0762, test_acc: 0.9789 \n",
            "\n",
            "epoch: 75/100, train_loss: 0.0037, train_acc: 1.0000, test_loss: 0.0762, test_acc: 0.9807 \n",
            "\n",
            "epoch: 76/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0769, test_acc: 0.9816 \n",
            "\n",
            "epoch: 77/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0760, test_acc: 0.9807 \n",
            "\n",
            "epoch: 78/100, train_loss: 0.0033, train_acc: 0.9998, test_loss: 0.0755, test_acc: 0.9807 \n",
            "\n",
            "epoch: 79/100, train_loss: 0.0037, train_acc: 1.0000, test_loss: 0.0759, test_acc: 0.9798 \n",
            "\n",
            "epoch: 80/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0770, test_acc: 0.9807 \n",
            "\n",
            "epoch: 81/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0757, test_acc: 0.9816 \n",
            "\n",
            "epoch: 82/100, train_loss: 0.0037, train_acc: 1.0000, test_loss: 0.0761, test_acc: 0.9816 \n",
            "\n",
            "epoch: 83/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0763, test_acc: 0.9816 \n",
            "\n",
            "epoch: 84/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0759, test_acc: 0.9798 \n",
            "\n",
            "epoch: 85/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0757, test_acc: 0.9807 \n",
            "\n",
            "epoch: 86/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0766, test_acc: 0.9798 \n",
            "\n",
            "epoch: 87/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0763, test_acc: 0.9833 \n",
            "\n",
            "epoch: 88/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0757, test_acc: 0.9807 \n",
            "\n",
            "epoch: 89/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0762, test_acc: 0.9825 \n",
            "\n",
            "epoch: 90/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0760, test_acc: 0.9816 \n",
            "\n",
            "epoch: 91/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0766, test_acc: 0.9825 \n",
            "\n",
            "epoch: 92/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0760, test_acc: 0.9816 \n",
            "\n",
            "epoch: 93/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0764, test_acc: 0.9825 \n",
            "\n",
            "epoch: 94/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0766, test_acc: 0.9807 \n",
            "\n",
            "epoch: 95/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0758, test_acc: 0.9816 \n",
            "\n",
            "epoch: 96/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0759, test_acc: 0.9825 \n",
            "\n",
            "epoch: 97/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0764, test_acc: 0.9833 \n",
            "\n",
            "epoch: 98/100, train_loss: 0.0033, train_acc: 1.0000, test_loss: 0.0756, test_acc: 0.9825 \n",
            "\n",
            "epoch: 99/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0763, test_acc: 0.9825 \n",
            "\n",
            "epoch: 100/100, train_loss: 0.0034, train_acc: 1.0000, test_loss: 0.0760, test_acc: 0.9798 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARg-WvTGM_GS",
        "colab_type": "text"
      },
      "source": [
        "Then I will try to modify the model. First, I want to  add a residual_layer.(v1.4)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Difference\n",
        "\n",
        "    def Build_SEnet(self, input_x):\n",
        "        # only cifar10 architecture\n",
        "\n",
        "        input_x = self.first_layer(input_x, scope='first_layer')\n",
        "\n",
        "        x = self.residual_layer(input_x, out_dim=64, layer_num='1')\n",
        "        x = self.residual_layer(x, out_dim=128, layer_num='2')\n",
        "        x = self.residual_layer(x, out_dim=256, layer_num='3')\n",
        "        x = self.residual_layer(x, out_dim=512, layer_num='4')\n",
        "        \n",
        "        x = Global_Average_Pooling(x)\n",
        "        x = flatten(x)\n",
        "\n",
        "        x = Fully_connected(x, layer_name='final_fully_connected')\n",
        "        return x\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7UqG3P2NWyq",
        "colab_type": "code",
        "outputId": "7e0573b3-6e83-48c8-ec51-64aa626445f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4283
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tflearn.layers.conv import global_avg_pool\n",
        "from tensorflow.contrib.layers import batch_norm, flatten\n",
        "from tensorflow.contrib.framework import arg_scope\n",
        "import keras.layers\n",
        "import numpy as np\n",
        "\n",
        "weight_decay = 0.0005\n",
        "momentum = 0.9\n",
        "\n",
        "init_learning_rate = 0.1\n",
        "cardinality = 8 # how many split ?\n",
        "blocks = 3 # res_block ! (split + transition)\n",
        "depth = 64 # out channel\n",
        "\n",
        "\"\"\"\n",
        "So, the total number of layers is (3*blokcs)*residual_layer_num + 2\n",
        "because, blocks = split(conv 2) + transition(conv 1) = 3 layer\n",
        "and, first conv layer 1, last dense layer 1\n",
        "thus, total number of layers = (3*blocks)*residual_layer_num + 2\n",
        "\"\"\"\n",
        "\n",
        "reduction_ratio = 4\n",
        "class_num=10\n",
        "batch_size = 128\n",
        "\n",
        "# iteration = 391\n",
        "# 128 * 391 ~ 50,000\n",
        "\n",
        "iteration=36\n",
        "# 128*36~4608\n",
        "\n",
        "test_iteration = 10\n",
        "\n",
        "total_epochs = 100\n",
        "\n",
        "def conv_layer(input, filter, kernel, stride, padding='SAME', layer_name=\"conv\"):\n",
        "    with tf.name_scope(layer_name):\n",
        "        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n",
        "        return network\n",
        "\n",
        "def Global_Average_Pooling(x):\n",
        "    return global_avg_pool(x, name='Global_avg_pooling')\n",
        "\n",
        "def Average_pooling(x, pool_size=[2,2], stride=2, padding='SAME'):\n",
        "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
        "\n",
        "def Batch_Normalization(x, training, scope):\n",
        "    with arg_scope([batch_norm],\n",
        "                   scope=scope,\n",
        "                   updates_collections=None,\n",
        "                   decay=0.9,\n",
        "                   center=True,\n",
        "                   scale=True,\n",
        "                   zero_debias_moving_mean=True) :\n",
        "        return tf.cond(training,\n",
        "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
        "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
        "\n",
        "def Relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "def Sigmoid(x) :\n",
        "    return tf.nn.sigmoid(x)\n",
        "\n",
        "def Concatenation(layers) :\n",
        "    return tf.concat(layers, axis=3)\n",
        "\n",
        "def Fully_connected(x, units=class_num, layer_name='fully_connected') :\n",
        "    with tf.name_scope(layer_name) :\n",
        "        return tf.layers.dense(inputs=x, use_bias=False, units=units)\n",
        "\n",
        "def Evaluate(sess):\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "    test_pre_index = 0\n",
        "    add = int(len(test_x)/test_iteration)\n",
        "\n",
        "    for it in range(test_iteration):\n",
        "        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n",
        "        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n",
        "        test_pre_index = test_pre_index + add\n",
        "\n",
        "        test_feed_dict = {\n",
        "            x: test_batch_x,\n",
        "            label: test_batch_y,\n",
        "            learning_rate: epoch_learning_rate,\n",
        "            training_flag: False\n",
        "        }\n",
        "\n",
        "        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n",
        "\n",
        "        test_loss += loss_\n",
        "        test_acc += acc_\n",
        "\n",
        "    test_loss /= test_iteration # average loss\n",
        "    test_acc /= test_iteration # average accuracy\n",
        "\n",
        "    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=test_loss),\n",
        "                                tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
        "\n",
        "    return test_acc, test_loss, summary\n",
        "\n",
        "class SE_ResNeXt():\n",
        "    def __init__(self, x, training):\n",
        "        self.training = training\n",
        "        self.model = self.Build_SEnet(x)\n",
        "\n",
        "    def first_layer(self, x, scope):\n",
        "        with tf.name_scope(scope) :\n",
        "            x = conv_layer(x, filter=64, kernel=[3, 3], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            x = Relu(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def transform_layer(self, x, stride, scope):\n",
        "        with tf.name_scope(scope) :\n",
        "            x = conv_layer(x, filter=depth, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            x = Relu(x)\n",
        "\n",
        "            x = conv_layer(x, filter=depth, kernel=[3,3], stride=stride, layer_name=scope+'_conv2')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch2')\n",
        "            x = Relu(x)\n",
        "            return x\n",
        "\n",
        "    def transition_layer(self, x, out_dim, scope):\n",
        "        with tf.name_scope(scope):\n",
        "            x = conv_layer(x, filter=out_dim, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            # x = Relu(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def split_layer(self, input_x, stride, layer_name):\n",
        "        with tf.name_scope(layer_name) :\n",
        "            layers_split = list()\n",
        "            for i in range(cardinality) :\n",
        "                splits = self.transform_layer(input_x, stride=stride, scope=layer_name + '_splitN_' + str(i))\n",
        "                layers_split.append(splits)\n",
        "\n",
        "            return Concatenation(layers_split)\n",
        "\n",
        "    def squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n",
        "        with tf.name_scope(layer_name) :\n",
        "\n",
        "\n",
        "            squeeze = Global_Average_Pooling(input_x)\n",
        "\n",
        "            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
        "            excitation = Relu(excitation)\n",
        "            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
        "            excitation = Sigmoid(excitation)\n",
        "\n",
        "            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
        "            scale = input_x * excitation\n",
        "\n",
        "            return scale\n",
        "\n",
        "    def residual_layer(self, input_x, out_dim, layer_num, res_block=blocks):\n",
        "        # split + transform(bottleneck) + transition + merge\n",
        "        # input_dim = input_x.get_shape().as_list()[-1]\n",
        "\n",
        "        for i in range(res_block):\n",
        "            input_dim = int(np.shape(input_x)[-1])\n",
        "\n",
        "            if input_dim * 2 == out_dim:\n",
        "                flag = True\n",
        "                stride = 2\n",
        "                channel = input_dim // 2\n",
        "            else:\n",
        "                flag = False\n",
        "                stride = 1\n",
        "\n",
        "            x = self.split_layer(input_x, stride=stride, layer_name='split_layer_'+layer_num+'_'+str(i))\n",
        "            x = self.transition_layer(x, out_dim=out_dim, scope='trans_layer_'+layer_num+'_'+str(i))\n",
        "            x = self.squeeze_excitation_layer(x, out_dim=out_dim, ratio=reduction_ratio, layer_name='squeeze_layer_'+layer_num+'_'+str(i))\n",
        "\n",
        "            if flag is True :\n",
        "                pad_input_x = Average_pooling(input_x)\n",
        "                pad_input_x = tf.pad(pad_input_x, [[0, 0], [0, 0], [0, 0], [channel, channel]]) # [?, height, width, channel]\n",
        "            else :\n",
        "                pad_input_x = input_x\n",
        "\n",
        "            input_x = Relu(x + pad_input_x)\n",
        "\n",
        "        return input_x\n",
        "\n",
        "\n",
        "    def Build_SEnet(self, input_x):\n",
        "        # only cifar10 architecture\n",
        "\n",
        "        input_x = self.first_layer(input_x, scope='first_layer')\n",
        "\n",
        "        x = self.residual_layer(input_x, out_dim=64, layer_num='1')\n",
        "        x = self.residual_layer(x, out_dim=128, layer_num='2')\n",
        "        x = self.residual_layer(x, out_dim=256, layer_num='3')\n",
        "        x = self.residual_layer(x, out_dim=512, layer_num='4')\n",
        "        \n",
        "        x = Global_Average_Pooling(x)\n",
        "        x = flatten(x)\n",
        "\n",
        "        x = Fully_connected(x, layer_name='final_fully_connected')\n",
        "        return x\n",
        "\n",
        "\n",
        "train_x, train_y, test_x, test_y = prepare_data()\n",
        "train_x, test_x = color_preprocessing(train_x, test_x)\n",
        "\n",
        "\n",
        "# image_size = 32, img_channels = 3, class_num = 10 in cifar10\n",
        "x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\n",
        "label = tf.placeholder(tf.float32, shape=[None, class_num])\n",
        "\n",
        "training_flag = tf.placeholder(tf.bool)\n",
        "\n",
        "\n",
        "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
        "\n",
        "logits = SE_ResNeXt(x, training=training_flag).model\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logits))\n",
        "\n",
        "l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
        "train = optimizer.minimize(cost + l2_loss * weight_decay)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess :\n",
        "    ckpt = tf.train.get_checkpoint_state('./model_v1_4')\n",
        "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter('./logs_v1_4', sess.graph)\n",
        "\n",
        "    epoch_learning_rate = init_learning_rate\n",
        "    for epoch in range(1, total_epochs + 1):\n",
        "        if epoch % 30 == 0 :\n",
        "            epoch_learning_rate = epoch_learning_rate / 10\n",
        "\n",
        "        pre_index = 0\n",
        "        train_acc = 0.0\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for step in range(1, iteration + 1):\n",
        "            if pre_index + batch_size < 4586:\n",
        "                batch_x = train_x[pre_index: pre_index + batch_size]\n",
        "                batch_y = train_y[pre_index: pre_index + batch_size]\n",
        "            else:\n",
        "                batch_x = train_x[pre_index:]\n",
        "                batch_y = train_y[pre_index:]\n",
        "\n",
        "            batch_x = data_augmentation(batch_x)\n",
        "\n",
        "            train_feed_dict = {\n",
        "                x: batch_x,\n",
        "                label: batch_y,\n",
        "                learning_rate: epoch_learning_rate,\n",
        "                training_flag: True\n",
        "            }\n",
        "\n",
        "            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n",
        "            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n",
        "\n",
        "            train_loss += batch_loss\n",
        "            train_acc += batch_acc\n",
        "            pre_index += batch_size\n",
        "\n",
        "\n",
        "        train_loss /= iteration # average loss\n",
        "        train_acc /= iteration # average accuracy\n",
        "\n",
        "        train_summary = tf.Summary(value=[tf.Summary.Value(tag='train_loss', simple_value=train_loss),\n",
        "                                          tf.Summary.Value(tag='train_accuracy', simple_value=train_acc)])\n",
        "\n",
        "        test_acc, test_loss, test_summary = Evaluate(sess)\n",
        "\n",
        "        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n",
        "        summary_writer.add_summary(summary=test_summary, global_step=epoch)\n",
        "        summary_writer.flush()\n",
        "\n",
        "        line = \"epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n\" % (\n",
        "            epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n",
        "        print(line)\n",
        "\n",
        "        with open('logs.txt', 'a') as f:\n",
        "            f.write(line)\n",
        "\n",
        "        saver.save(sess=sess, save_path='./model_v1_4/ResNeXt_v1_4.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 03:01:58.283171 140268881917824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0614 03:01:58.284458 140268881917824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0614 03:01:58.300127 140268881917824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0614 03:01:58.309382 140268881917824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0614 03:01:58.322672 140268881917824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0614 03:01:58.323537 140268881917824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "W0614 03:01:59.236195 140268881917824 deprecation.py:323] From <ipython-input-5-48f3d3848dcc>:39: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0614 03:01:59.241410 140268881917824 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0614 03:02:02.231228 140268881917824 deprecation.py:323] From <ipython-input-5-48f3d3848dcc>:71: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0614 03:02:10.063141 140268881917824 deprecation.py:323] From <ipython-input-5-48f3d3848dcc>:46: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.AveragePooling2D instead.\n",
            "W0614 03:02:30.118155 140268881917824 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1/100, train_loss: 2.3858, train_acc: 0.5124, test_loss: 4.8741, test_acc: 0.5281 \n",
            "\n",
            "epoch: 2/100, train_loss: 0.9540, train_acc: 0.7246, test_loss: 0.7517, test_acc: 0.7246 \n",
            "\n",
            "epoch: 3/100, train_loss: 0.7260, train_acc: 0.7929, test_loss: 0.9107, test_acc: 0.7386 \n",
            "\n",
            "epoch: 4/100, train_loss: 0.6091, train_acc: 0.8394, test_loss: 0.6927, test_acc: 0.8070 \n",
            "\n",
            "epoch: 5/100, train_loss: 0.5034, train_acc: 0.8749, test_loss: 1.6319, test_acc: 0.7982 \n",
            "\n",
            "epoch: 6/100, train_loss: 0.4252, train_acc: 0.9072, test_loss: 0.5680, test_acc: 0.7868 \n",
            "\n",
            "epoch: 7/100, train_loss: 0.3835, train_acc: 0.9188, test_loss: 0.8279, test_acc: 0.7360 \n",
            "\n",
            "epoch: 8/100, train_loss: 0.3374, train_acc: 0.9347, test_loss: 0.4425, test_acc: 0.8623 \n",
            "\n",
            "epoch: 9/100, train_loss: 0.2989, train_acc: 0.9490, test_loss: 0.4798, test_acc: 0.8351 \n",
            "\n",
            "epoch: 10/100, train_loss: 0.2607, train_acc: 0.9656, test_loss: 0.4250, test_acc: 0.8430 \n",
            "\n",
            "epoch: 11/100, train_loss: 0.2380, train_acc: 0.9691, test_loss: 0.3194, test_acc: 0.8912 \n",
            "\n",
            "epoch: 12/100, train_loss: 0.2052, train_acc: 0.9758, test_loss: 0.4427, test_acc: 0.8491 \n",
            "\n",
            "epoch: 13/100, train_loss: 0.2116, train_acc: 0.9775, test_loss: 0.4185, test_acc: 0.8605 \n",
            "\n",
            "epoch: 14/100, train_loss: 0.2594, train_acc: 0.9703, test_loss: 0.3965, test_acc: 0.8579 \n",
            "\n",
            "epoch: 15/100, train_loss: 0.1676, train_acc: 0.9868, test_loss: 0.3627, test_acc: 0.8675 \n",
            "\n",
            "epoch: 16/100, train_loss: 0.1421, train_acc: 0.9896, test_loss: 0.3264, test_acc: 0.8939 \n",
            "\n",
            "epoch: 17/100, train_loss: 0.1449, train_acc: 0.9902, test_loss: 0.3350, test_acc: 0.8921 \n",
            "\n",
            "epoch: 18/100, train_loss: 0.1456, train_acc: 0.9931, test_loss: 1.9846, test_acc: 0.6711 \n",
            "\n",
            "epoch: 19/100, train_loss: 0.1281, train_acc: 0.9919, test_loss: 1.0586, test_acc: 0.7711 \n",
            "\n",
            "epoch: 20/100, train_loss: 0.1522, train_acc: 0.9918, test_loss: 0.3717, test_acc: 0.8737 \n",
            "\n",
            "epoch: 21/100, train_loss: 0.1013, train_acc: 0.9963, test_loss: 0.2734, test_acc: 0.9018 \n",
            "\n",
            "epoch: 22/100, train_loss: 0.0859, train_acc: 0.9976, test_loss: 0.2188, test_acc: 0.9307 \n",
            "\n",
            "epoch: 23/100, train_loss: 0.0868, train_acc: 0.9976, test_loss: 0.6966, test_acc: 0.8605 \n",
            "\n",
            "epoch: 24/100, train_loss: 0.1207, train_acc: 0.9947, test_loss: 1.3025, test_acc: 0.8026 \n",
            "\n",
            "epoch: 25/100, train_loss: 0.1235, train_acc: 0.9935, test_loss: 0.7369, test_acc: 0.7982 \n",
            "\n",
            "epoch: 26/100, train_loss: 0.0830, train_acc: 0.9970, test_loss: 0.3892, test_acc: 0.8877 \n",
            "\n",
            "epoch: 27/100, train_loss: 0.0811, train_acc: 0.9974, test_loss: 0.3403, test_acc: 0.8904 \n",
            "\n",
            "epoch: 28/100, train_loss: 0.0689, train_acc: 0.9987, test_loss: 1.0416, test_acc: 0.8351 \n",
            "\n",
            "epoch: 29/100, train_loss: 0.0832, train_acc: 0.9976, test_loss: 0.3667, test_acc: 0.9035 \n",
            "\n",
            "epoch: 30/100, train_loss: 0.0461, train_acc: 0.9883, test_loss: 0.0959, test_acc: 0.9675 \n",
            "\n",
            "epoch: 31/100, train_loss: 0.0178, train_acc: 0.9978, test_loss: 0.0862, test_acc: 0.9737 \n",
            "\n",
            "epoch: 32/100, train_loss: 0.0143, train_acc: 0.9985, test_loss: 0.0831, test_acc: 0.9781 \n",
            "\n",
            "epoch: 33/100, train_loss: 0.0109, train_acc: 0.9996, test_loss: 0.0811, test_acc: 0.9816 \n",
            "\n",
            "epoch: 34/100, train_loss: 0.0103, train_acc: 0.9991, test_loss: 0.0763, test_acc: 0.9798 \n",
            "\n",
            "epoch: 35/100, train_loss: 0.0092, train_acc: 0.9993, test_loss: 0.0756, test_acc: 0.9825 \n",
            "\n",
            "epoch: 36/100, train_loss: 0.0079, train_acc: 0.9996, test_loss: 0.0757, test_acc: 0.9789 \n",
            "\n",
            "epoch: 37/100, train_loss: 0.0083, train_acc: 0.9998, test_loss: 0.0737, test_acc: 0.9807 \n",
            "\n",
            "epoch: 38/100, train_loss: 0.0066, train_acc: 0.9998, test_loss: 0.0733, test_acc: 0.9833 \n",
            "\n",
            "epoch: 39/100, train_loss: 0.0064, train_acc: 0.9998, test_loss: 0.0715, test_acc: 0.9833 \n",
            "\n",
            "epoch: 40/100, train_loss: 0.0059, train_acc: 0.9998, test_loss: 0.0731, test_acc: 0.9851 \n",
            "\n",
            "epoch: 41/100, train_loss: 0.0054, train_acc: 1.0000, test_loss: 0.0732, test_acc: 0.9851 \n",
            "\n",
            "epoch: 42/100, train_loss: 0.0048, train_acc: 1.0000, test_loss: 0.0726, test_acc: 0.9851 \n",
            "\n",
            "epoch: 43/100, train_loss: 0.0046, train_acc: 0.9998, test_loss: 0.0727, test_acc: 0.9860 \n",
            "\n",
            "epoch: 44/100, train_loss: 0.0045, train_acc: 1.0000, test_loss: 0.0720, test_acc: 0.9860 \n",
            "\n",
            "epoch: 45/100, train_loss: 0.0041, train_acc: 1.0000, test_loss: 0.0707, test_acc: 0.9860 \n",
            "\n",
            "epoch: 46/100, train_loss: 0.0043, train_acc: 0.9998, test_loss: 0.0676, test_acc: 0.9868 \n",
            "\n",
            "epoch: 47/100, train_loss: 0.0039, train_acc: 1.0000, test_loss: 0.0699, test_acc: 0.9851 \n",
            "\n",
            "epoch: 48/100, train_loss: 0.0036, train_acc: 1.0000, test_loss: 0.0699, test_acc: 0.9868 \n",
            "\n",
            "epoch: 49/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0658, test_acc: 0.9868 \n",
            "\n",
            "epoch: 50/100, train_loss: 0.0039, train_acc: 0.9996, test_loss: 0.0727, test_acc: 0.9851 \n",
            "\n",
            "epoch: 51/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0692, test_acc: 0.9868 \n",
            "\n",
            "epoch: 52/100, train_loss: 0.0036, train_acc: 1.0000, test_loss: 0.0678, test_acc: 0.9868 \n",
            "\n",
            "epoch: 53/100, train_loss: 0.0028, train_acc: 1.0000, test_loss: 0.0702, test_acc: 0.9868 \n",
            "\n",
            "epoch: 54/100, train_loss: 0.0035, train_acc: 1.0000, test_loss: 0.0700, test_acc: 0.9833 \n",
            "\n",
            "epoch: 55/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0707, test_acc: 0.9886 \n",
            "\n",
            "epoch: 56/100, train_loss: 0.0031, train_acc: 1.0000, test_loss: 0.0693, test_acc: 0.9868 \n",
            "\n",
            "epoch: 57/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0712, test_acc: 0.9877 \n",
            "\n",
            "epoch: 58/100, train_loss: 0.0029, train_acc: 1.0000, test_loss: 0.0690, test_acc: 0.9868 \n",
            "\n",
            "epoch: 59/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0701, test_acc: 0.9877 \n",
            "\n",
            "epoch: 60/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0674, test_acc: 0.9868 \n",
            "\n",
            "epoch: 61/100, train_loss: 0.0032, train_acc: 0.9996, test_loss: 0.0693, test_acc: 0.9877 \n",
            "\n",
            "epoch: 62/100, train_loss: 0.0026, train_acc: 1.0000, test_loss: 0.0688, test_acc: 0.9877 \n",
            "\n",
            "epoch: 63/100, train_loss: 0.0027, train_acc: 1.0000, test_loss: 0.0689, test_acc: 0.9877 \n",
            "\n",
            "epoch: 64/100, train_loss: 0.0026, train_acc: 1.0000, test_loss: 0.0672, test_acc: 0.9877 \n",
            "\n",
            "epoch: 65/100, train_loss: 0.0026, train_acc: 1.0000, test_loss: 0.0674, test_acc: 0.9877 \n",
            "\n",
            "epoch: 66/100, train_loss: 0.0026, train_acc: 1.0000, test_loss: 0.0656, test_acc: 0.9877 \n",
            "\n",
            "epoch: 67/100, train_loss: 0.0032, train_acc: 1.0000, test_loss: 0.0669, test_acc: 0.9877 \n",
            "\n",
            "epoch: 68/100, train_loss: 0.0026, train_acc: 1.0000, test_loss: 0.0672, test_acc: 0.9877 \n",
            "\n",
            "epoch: 69/100, train_loss: 0.0026, train_acc: 1.0000, test_loss: 0.0669, test_acc: 0.9877 \n",
            "\n",
            "epoch: 70/100, train_loss: 0.0024, train_acc: 1.0000, test_loss: 0.0678, test_acc: 0.9877 \n",
            "\n",
            "epoch: 71/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0665, test_acc: 0.9877 \n",
            "\n",
            "epoch: 72/100, train_loss: 0.0027, train_acc: 0.9998, test_loss: 0.0677, test_acc: 0.9877 \n",
            "\n",
            "epoch: 73/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0668, test_acc: 0.9877 \n",
            "\n",
            "epoch: 74/100, train_loss: 0.0029, train_acc: 0.9998, test_loss: 0.0666, test_acc: 0.9877 \n",
            "\n",
            "epoch: 75/100, train_loss: 0.0026, train_acc: 1.0000, test_loss: 0.0670, test_acc: 0.9877 \n",
            "\n",
            "epoch: 76/100, train_loss: 0.0029, train_acc: 1.0000, test_loss: 0.0665, test_acc: 0.9877 \n",
            "\n",
            "epoch: 77/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0660, test_acc: 0.9877 \n",
            "\n",
            "epoch: 78/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0679, test_acc: 0.9877 \n",
            "\n",
            "epoch: 79/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0668, test_acc: 0.9877 \n",
            "\n",
            "epoch: 80/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0672, test_acc: 0.9877 \n",
            "\n",
            "epoch: 81/100, train_loss: 0.0027, train_acc: 1.0000, test_loss: 0.0676, test_acc: 0.9877 \n",
            "\n",
            "epoch: 82/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0676, test_acc: 0.9877 \n",
            "\n",
            "epoch: 83/100, train_loss: 0.0028, train_acc: 1.0000, test_loss: 0.0672, test_acc: 0.9877 \n",
            "\n",
            "epoch: 84/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0675, test_acc: 0.9877 \n",
            "\n",
            "epoch: 85/100, train_loss: 0.0024, train_acc: 1.0000, test_loss: 0.0663, test_acc: 0.9877 \n",
            "\n",
            "epoch: 86/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0673, test_acc: 0.9877 \n",
            "\n",
            "epoch: 87/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0666, test_acc: 0.9877 \n",
            "\n",
            "epoch: 88/100, train_loss: 0.0035, train_acc: 0.9996, test_loss: 0.0676, test_acc: 0.9877 \n",
            "\n",
            "epoch: 89/100, train_loss: 0.0024, train_acc: 1.0000, test_loss: 0.0670, test_acc: 0.9877 \n",
            "\n",
            "epoch: 90/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0680, test_acc: 0.9877 \n",
            "\n",
            "epoch: 91/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0676, test_acc: 0.9877 \n",
            "\n",
            "epoch: 92/100, train_loss: 0.0024, train_acc: 1.0000, test_loss: 0.0690, test_acc: 0.9877 \n",
            "\n",
            "epoch: 93/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0660, test_acc: 0.9886 \n",
            "\n",
            "epoch: 94/100, train_loss: 0.0027, train_acc: 1.0000, test_loss: 0.0680, test_acc: 0.9877 \n",
            "\n",
            "epoch: 95/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0682, test_acc: 0.9877 \n",
            "\n",
            "epoch: 96/100, train_loss: 0.0023, train_acc: 1.0000, test_loss: 0.0681, test_acc: 0.9877 \n",
            "\n",
            "epoch: 97/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0673, test_acc: 0.9877 \n",
            "\n",
            "epoch: 98/100, train_loss: 0.0025, train_acc: 1.0000, test_loss: 0.0681, test_acc: 0.9877 \n",
            "\n",
            "epoch: 99/100, train_loss: 0.0026, train_acc: 1.0000, test_loss: 0.0682, test_acc: 0.9877 \n",
            "\n",
            "epoch: 100/100, train_loss: 0.0024, train_acc: 1.0000, test_loss: 0.0659, test_acc: 0.9877 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWkDPmZ0-JJq",
        "colab_type": "text"
      },
      "source": [
        "Then, I want to  decrease a residual_layer based on the original model.(v1.5)\n",
        "\n",
        "\n",
        "```\n",
        "# Difference\n",
        "\n",
        "    def Build_SEnet(self, input_x):\n",
        "        # only cifar10 architecture\n",
        "\n",
        "        input_x = self.first_layer(input_x, scope='first_layer')\n",
        "\n",
        "        x = self.residual_layer(input_x, out_dim=64, layer_num='1')\n",
        "        x = self.residual_layer(x, out_dim=128, layer_num='2')\n",
        "\n",
        "        \n",
        "        x = Global_Average_Pooling(x)\n",
        "        x = flatten(x)\n",
        "\n",
        "        x = Fully_connected(x, layer_name='final_fully_connected')\n",
        "        return x\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLyhfCrpY6cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tflearn.layers.conv import global_avg_pool\n",
        "from tensorflow.contrib.layers import batch_norm, flatten\n",
        "from tensorflow.contrib.framework import arg_scope\n",
        "import keras.layers\n",
        "import numpy as np\n",
        "\n",
        "weight_decay = 0.0005\n",
        "momentum = 0.9\n",
        "\n",
        "init_learning_rate = 0.1\n",
        "cardinality = 8 # how many split ?\n",
        "blocks = 3 # res_block ! (split + transition)\n",
        "depth = 64 # out channel\n",
        "\n",
        "\"\"\"\n",
        "So, the total number of layers is (3*blokcs)*residual_layer_num + 2\n",
        "because, blocks = split(conv 2) + transition(conv 1) = 3 layer\n",
        "and, first conv layer 1, last dense layer 1\n",
        "thus, total number of layers = (3*blocks)*residual_layer_num + 2\n",
        "\"\"\"\n",
        "\n",
        "reduction_ratio = 4\n",
        "class_num=10\n",
        "batch_size = 128\n",
        "\n",
        "# iteration = 391\n",
        "# 128 * 391 ~ 50,000\n",
        "\n",
        "iteration=36\n",
        "# 128*36~4608\n",
        "\n",
        "test_iteration = 10\n",
        "\n",
        "total_epochs = 100\n",
        "\n",
        "def conv_layer(input, filter, kernel, stride, padding='SAME', layer_name=\"conv\"):\n",
        "    with tf.name_scope(layer_name):\n",
        "        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n",
        "        return network\n",
        "\n",
        "def Global_Average_Pooling(x):\n",
        "    return global_avg_pool(x, name='Global_avg_pooling')\n",
        "\n",
        "def Average_pooling(x, pool_size=[2,2], stride=2, padding='SAME'):\n",
        "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
        "\n",
        "def Batch_Normalization(x, training, scope):\n",
        "    with arg_scope([batch_norm],\n",
        "                   scope=scope,\n",
        "                   updates_collections=None,\n",
        "                   decay=0.9,\n",
        "                   center=True,\n",
        "                   scale=True,\n",
        "                   zero_debias_moving_mean=True) :\n",
        "        return tf.cond(training,\n",
        "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
        "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
        "\n",
        "def Relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "def Sigmoid(x) :\n",
        "    return tf.nn.sigmoid(x)\n",
        "\n",
        "def Concatenation(layers) :\n",
        "    return tf.concat(layers, axis=3)\n",
        "\n",
        "def Fully_connected(x, units=class_num, layer_name='fully_connected') :\n",
        "    with tf.name_scope(layer_name) :\n",
        "        return tf.layers.dense(inputs=x, use_bias=False, units=units)\n",
        "\n",
        "def Evaluate(sess):\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "    test_pre_index = 0\n",
        "    add = int(len(test_x)/test_iteration)\n",
        "\n",
        "    for it in range(test_iteration):\n",
        "        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n",
        "        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n",
        "        test_pre_index = test_pre_index + add\n",
        "\n",
        "        test_feed_dict = {\n",
        "            x: test_batch_x,\n",
        "            label: test_batch_y,\n",
        "            learning_rate: epoch_learning_rate,\n",
        "            training_flag: False\n",
        "        }\n",
        "\n",
        "        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n",
        "\n",
        "        test_loss += loss_\n",
        "        test_acc += acc_\n",
        "\n",
        "    test_loss /= test_iteration # average loss\n",
        "    test_acc /= test_iteration # average accuracy\n",
        "\n",
        "    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=test_loss),\n",
        "                                tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
        "\n",
        "    return test_acc, test_loss, summary\n",
        "\n",
        "class SE_ResNeXt():\n",
        "    def __init__(self, x, training):\n",
        "        self.training = training\n",
        "        self.model = self.Build_SEnet(x)\n",
        "\n",
        "    def first_layer(self, x, scope):\n",
        "        with tf.name_scope(scope) :\n",
        "            x = conv_layer(x, filter=64, kernel=[3, 3], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            x = Relu(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def transform_layer(self, x, stride, scope):\n",
        "        with tf.name_scope(scope) :\n",
        "            x = conv_layer(x, filter=depth, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            x = Relu(x)\n",
        "\n",
        "            x = conv_layer(x, filter=depth, kernel=[3,3], stride=stride, layer_name=scope+'_conv2')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch2')\n",
        "            x = Relu(x)\n",
        "            return x\n",
        "\n",
        "    def transition_layer(self, x, out_dim, scope):\n",
        "        with tf.name_scope(scope):\n",
        "            x = conv_layer(x, filter=out_dim, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
        "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
        "            # x = Relu(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def split_layer(self, input_x, stride, layer_name):\n",
        "        with tf.name_scope(layer_name) :\n",
        "            layers_split = list()\n",
        "            for i in range(cardinality) :\n",
        "                splits = self.transform_layer(input_x, stride=stride, scope=layer_name + '_splitN_' + str(i))\n",
        "                layers_split.append(splits)\n",
        "\n",
        "            return Concatenation(layers_split)\n",
        "\n",
        "    def squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n",
        "        with tf.name_scope(layer_name) :\n",
        "\n",
        "\n",
        "            squeeze = Global_Average_Pooling(input_x)\n",
        "\n",
        "            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
        "            excitation = Relu(excitation)\n",
        "            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
        "            excitation = Sigmoid(excitation)\n",
        "\n",
        "            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
        "            scale = input_x * excitation\n",
        "\n",
        "            return scale\n",
        "\n",
        "    def residual_layer(self, input_x, out_dim, layer_num, res_block=blocks):\n",
        "        # split + transform(bottleneck) + transition + merge\n",
        "        # input_dim = input_x.get_shape().as_list()[-1]\n",
        "\n",
        "        for i in range(res_block):\n",
        "            input_dim = int(np.shape(input_x)[-1])\n",
        "\n",
        "            if input_dim * 2 == out_dim:\n",
        "                flag = True\n",
        "                stride = 2\n",
        "                channel = input_dim // 2\n",
        "            else:\n",
        "                flag = False\n",
        "                stride = 1\n",
        "\n",
        "            x = self.split_layer(input_x, stride=stride, layer_name='split_layer_'+layer_num+'_'+str(i))\n",
        "            x = self.transition_layer(x, out_dim=out_dim, scope='trans_layer_'+layer_num+'_'+str(i))\n",
        "            x = self.squeeze_excitation_layer(x, out_dim=out_dim, ratio=reduction_ratio, layer_name='squeeze_layer_'+layer_num+'_'+str(i))\n",
        "\n",
        "            if flag is True :\n",
        "                pad_input_x = Average_pooling(input_x)\n",
        "                pad_input_x = tf.pad(pad_input_x, [[0, 0], [0, 0], [0, 0], [channel, channel]]) # [?, height, width, channel]\n",
        "            else :\n",
        "                pad_input_x = input_x\n",
        "\n",
        "            input_x = Relu(x + pad_input_x)\n",
        "\n",
        "        return input_x\n",
        "\n",
        "\n",
        "    def Build_SEnet(self, input_x):\n",
        "        # only cifar10 architecture\n",
        "\n",
        "        input_x = self.first_layer(input_x, scope='first_layer')\n",
        "\n",
        "        x = self.residual_layer(input_x, out_dim=64, layer_num='1')\n",
        "        x = self.residual_layer(x, out_dim=128, layer_num='2')\n",
        "\n",
        "\n",
        "        x = Global_Average_Pooling(x)\n",
        "        x = flatten(x)\n",
        "\n",
        "        x = Fully_connected(x, layer_name='final_fully_connected')\n",
        "        return x\n",
        "\n",
        "\n",
        "train_x, train_y, test_x, test_y = prepare_data()\n",
        "train_x, test_x = color_preprocessing(train_x, test_x)\n",
        "\n",
        "\n",
        "# image_size = 32, img_channels = 3, class_num = 10 in cifar10\n",
        "x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\n",
        "label = tf.placeholder(tf.float32, shape=[None, class_num])\n",
        "\n",
        "training_flag = tf.placeholder(tf.bool)\n",
        "\n",
        "\n",
        "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
        "\n",
        "logits = SE_ResNeXt(x, training=training_flag).model\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logits))\n",
        "\n",
        "l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
        "train = optimizer.minimize(cost + l2_loss * weight_decay)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess :\n",
        "    ckpt = tf.train.get_checkpoint_state('./model_v1_5')\n",
        "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter('./logs_v1_5', sess.graph)\n",
        "\n",
        "    epoch_learning_rate = init_learning_rate\n",
        "    for epoch in range(1, total_epochs + 1):\n",
        "        if epoch % 30 == 0 :\n",
        "            epoch_learning_rate = epoch_learning_rate / 10\n",
        "\n",
        "        pre_index = 0\n",
        "        train_acc = 0.0\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for step in range(1, iteration + 1):\n",
        "            if pre_index + batch_size < 4586:\n",
        "                batch_x = train_x[pre_index: pre_index + batch_size]\n",
        "                batch_y = train_y[pre_index: pre_index + batch_size]\n",
        "            else:\n",
        "                batch_x = train_x[pre_index:]\n",
        "                batch_y = train_y[pre_index:]\n",
        "\n",
        "            batch_x = data_augmentation(batch_x)\n",
        "\n",
        "            train_feed_dict = {\n",
        "                x: batch_x,\n",
        "                label: batch_y,\n",
        "                learning_rate: epoch_learning_rate,\n",
        "                training_flag: True\n",
        "            }\n",
        "\n",
        "            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n",
        "            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n",
        "\n",
        "            train_loss += batch_loss\n",
        "            train_acc += batch_acc\n",
        "            pre_index += batch_size\n",
        "\n",
        "\n",
        "        train_loss /= iteration # average loss\n",
        "        train_acc /= iteration # average accuracy\n",
        "\n",
        "        train_summary = tf.Summary(value=[tf.Summary.Value(tag='train_loss', simple_value=train_loss),\n",
        "                                          tf.Summary.Value(tag='train_accuracy', simple_value=train_acc)])\n",
        "\n",
        "        test_acc, test_loss, test_summary = Evaluate(sess)\n",
        "\n",
        "        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n",
        "        summary_writer.add_summary(summary=test_summary, global_step=epoch)\n",
        "        summary_writer.flush()\n",
        "\n",
        "        line = \"epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n\" % (\n",
        "            epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n",
        "        print(line)\n",
        "\n",
        "        with open('logs.txt', 'a') as f:\n",
        "            f.write(line)\n",
        "\n",
        "        saver.save(sess=sess, save_path='./model_v1_5/ResNeXt_v1_5.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSjJxIP4MjNz",
        "colab_type": "text"
      },
      "source": [
        "The following cell is to compress the model we trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFqYCcGwwzfy",
        "colab_type": "code",
        "outputId": "dcc2ccb0-f74d-4e6b-e8a7-ac206355434e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!zip -r ResNeXt_model.zip model\n",
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: model/ (stored 0%)\n",
            "  adding: model/ResNeXt.ckpt.meta (deflated 95%)\n",
            "  adding: model/ResNeXt.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "  adding: model/ResNeXt.ckpt.index (deflated 76%)\n",
            "  adding: model/checkpoint (deflated 40%)\n",
            "total 36896\n",
            "drwxr-xr-x 2 root root     4096 May 31 13:26 logs\n",
            "-rw-r--r-- 1 root root     9092 May 31 14:32 logs.txt\n",
            "drwxr-xr-x 2 root root     4096 May 31 14:32 model\n",
            "drwxr-xr-x 2 root root   163840 May 30 21:41 ok\n",
            "-rw-r--r-- 1 root root  5510451 May 31 13:23 ok.zip\n",
            "-rw-r--r-- 1 root root 32079501 May 31 16:49 ResNeXt_model.zip\n",
            "drwxr-xr-x 1 root root     4096 May 24 16:08 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cguvpNQsPByT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2aff53bf-ddcd-4e05-ec49-fb7e1a346c0b"
      },
      "source": [
        "!zip -r ResNeXt_model_v1_4.zip model_v1_4\n",
        "!ls -l"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: model_v1_4/ (stored 0%)\n",
            "  adding: model_v1_4/ResNeXt_v1_4.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "  adding: model_v1_4/ResNeXt_v1_4.ckpt.meta (deflated 95%)\n",
            "  adding: model_v1_4/ResNeXt_v1_4.ckpt.index (deflated 76%)\n",
            "  adding: model_v1_4/checkpoint (deflated 41%)\n",
            "total 57096\n",
            "-rw-r--r-- 1 root root     9092 Jun 14 04:18 logs.txt\n",
            "drwxr-xr-x 2 root root     4096 Jun 14 03:03 logs_v1_4\n",
            "drwxr-xr-x 2 root root     4096 Jun 14 04:18 model_v1_4\n",
            "drwxr-xr-x 2 root root   147456 May 30 21:41 ok\n",
            "-rw-r--r-- 1 root root  5510451 Jun 14 03:01 ok.zip\n",
            "-rw-r--r-- 1 root root 52777228 Jun 14 04:21 ResNeXt_model_v1_4.zip\n",
            "drwxr-xr-x 1 root root     4096 May 31 16:17 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZrmaMwLAsEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r ResNeXt_model_v1_5.zip model_v1_5\n",
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7z0skhAMiRK",
        "colab_type": "text"
      },
      "source": [
        "The following cell is to upload the model to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdt9KewgwjVJ",
        "colab_type": "code",
        "outputId": "e5740335-9901-4ec0-ec08-f413607b71ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a text file.\n",
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('ResNeXt_model.zip')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "# Uploaded file with ID 1zDwXJxuLPkODAG99Yvd7VLCqihPTKkUS"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 3.4MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Uploaded file with ID 1zDwXJxuLPkODAG99Yvd7VLCqihPTKkUS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_M2AOrVPLE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "77d436bc-1ffa-4aa6-aa1d-4d3e3918a64c"
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a text file.\n",
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('ResNeXt_model_v1_4.zip')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "Uploaded file with ID 1nIywPp_ud3DV5J7LVbFxvYjqlovoPES-"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0614 04:23:16.609454 140268881917824 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1nIywPp_ud3DV5J7LVbFxvYjqlovoPES-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wj_CupxA7dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a text file.\n",
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('ResNeXt_model_v1_5.zip')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "# Uploaded file with ID (Fill in this ID)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}